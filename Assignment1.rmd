#Datascience Capstone Assignment #1
###A.Ritz 6/2016


###Introduction
For this assignment / exercise, we will be performing a basic analysis on our three source files (english)
to get an idea of their respective sizes and content.

```{r, echo=FALSE, warning=FALSE}

rm(list=ls())

library(quanteda)
library(ggplot2)
library(dplyr)

# Let's grab our files to start.
dir<-'./DataFiles/en_US'
files<-list.files(dir, full.names = TRUE)

# How big are they (mb)?
fSizes<-sapply(files, function(x) paste0(file.size(x) / 1e6, 'Mb'))
print(fSizes)

```

###Analysis

In total, the file sizes are in excess of 500Mb, which is a rather large amount of data.  We can expect that there will similarly be a large number of lines + individual words.  We will collect that information now.

```{r cache=TRUE, warning=FALSE}

# We will define + apply a function that will be used to extract all of the lines form our files.
# We will use these lines for subsequent processing.
readAllLines<-function(path)
{
  c<-file(path, open="rt")
  lines<-readLines(c)
  close(c)
  
  lines
}

# This will take a while...
lineGroups <- sapply(files, readAllLines)

# Now we can get an idea of how many lines per file.
lineCounts<-sapply(lineGroups, length)
print(lineCounts)

most<-max(lineCounts)
total<-sum(lineCounts)

```

We can see that each of the files contain a large number of lines, the smallest of which is almost 900,000 lines, and the largest (twitter) has a count of **`r most`**.  In total, we have **`r total`** lines.
  
Now that we have all of our lines in place, we will tokenize them so that we can count the words on each of the lines, and later we will use these tokenized versions to compute n-grams so that we can get some idea of their distribution in our files.

```{r cache=TRUE}

# This will take all lines, squash them to a single vector, and tokenize them.
getTokens<-function(lines)
{
  doc<-paste(lines, collapse = " ")
  
  # Note the removal of punctuation, numbers, etc. from the list of tokens.
  t<-tokenize(doc, removeNumbers=TRUE, removePunct=TRUE, removeTwitter=TRUE, simplify=TRUE)
  t
}

# Before we begin counting ngrams, we will want to count the total numer of words in all docs.
# Because of the size, this will also take a while.  Each of the line sets are collapsed into
# a single 'document' so that we may more effectively tokenize it.
countWords<-function(lines)
{
  t<-getTokens(lines)

  # a tokenized document is simply a list of all tokens (words).  So if we take its length,
  # we get the word count.
  res<-length(t)
  res
}

wordCounts<-sapply(lineGroups, countWords)
wordSum<-sum(wordCounts)
print(wordCounts)

```

As we can see, there are a huge number of words in all of the documents.
-- Make note of which doc has the most.
The combined count is almost 100 million, weighing in at **`r wordSum`**

```{r cache=TRUE}
# Because of the library that we are using (quanteda) we will combine all of the lines into a
# single character array (this will make tabulation easier later).
# of course, we don't want to include all of the data because of the sheer size of it.  Instead,
# we will take a sample, and infer some numbers from it.

set.seed(1234)
sampleSize<-1000
sampleSet<- sapply(lineGroups, function(x) sample(x, sampleSize))

COLS<-2


# collapse our matrix into a single document composed of our sampled data.
# once we have the docs together, we can tokenize!
docs<-apply(sampleSet, MARGIN=COLS, function(x) paste(x, collapse = " "))
docs<-toLower(paste(docs, collapse=" "))

# We still want to remove special chars, etc.
docTokens<-tokenize(docs, removeNumbers=TRUE, removePunct=TRUE, removeTwitter=TRUE, simplify=TRUE)

tokenDF<-data.frame(table(docTokens))
names(tokenDF) <-c('Term', 'Count')


```

```{r}
# Common plotting function.
# Takes care of sorting + topN selection.
# Returns a ggplot object that must be printed.
# Please note that we are using 'Term' instead of n-gram (unigram, bigram, trigram) for ease of coding
# the labels will make the size of the ngrams obvious.
TOP_N<-25
plotnGrams<-function(df)
{
  byCount<-arrange(df, desc(Count))[1:TOP_N,]

  g <- ggplot(byCount, aes(x=reorder(Term, Count), y=Count)) +
    geom_bar(stat = "identity") + 
    coord_flip() +
    theme(legend.title=element_blank()) +
    xlab("Term") + 
    ylab("Count") +
    labs(title = "Top Terms by Count")
  g
}
```

```{r fig.cap="Tokens by Frequency"}

# Now we plot and print!

g1<-plotnGrams(tokenDF)
print(g1)

# Compute a quick stat to convey proportion of tokens.
topSum<-sum((arrange(tokenDF, desc(Count))[1:TOP_N,])$Count)
topPct<-(topSum/length(docTokens))*100

```

We can see here that most of the words from our sample are what one would expect.  Lots the smaller, more common words.  Technically these are known as "[stop words](https://en.wikipedia.org/wiki/Stop_words)".  During the tokenization process we could leave them out as they may not convey as much meaning as we would like, and because of their commonality (**`r topPct`%** for the first **`r TOP_N`** terms) they may make our predictions more difficult.  I have left them in or this analysis however.

##Bigrams and Trigrams
As a final step of our analysis, we are going to measure the bigrams and trigrams some our sample.  These correspond to n-grams of size two and three respectively.

```{r, cache=TRUE}

# We already have our sample tokens, so all we have to do is compute the grams, and get them in shape for
# plotting.
bigrams<-ngrams(docTokens, n=2, concatenator=" ")
bgDF<-data.frame(table(bigrams))
names(bgDF) <-c('Term', 'Count')
g2<-plotnGrams(bgDF)

trigrams<-ngrams(docTokens, n=3, concatenator=" ")
tgDF<-data.frame(table(trigrams))
names(tgDF) <-c('Term', 'Count')
g3<-plotnGrams(tgDF)


```

```{r fig.cap="Plot of frequent bigram Data"}

print(g2)

```

```{r fig.cap="Plot of frequent Trigram Data"}

print(g3)

```


###Sumary
As one might expect, the relative frequency of the n-grams drops sharply as we add more tokens to the n-grams.
This looks like it may be a useful feature as it could make it easier to predict the next word after a user has already input two or more.


